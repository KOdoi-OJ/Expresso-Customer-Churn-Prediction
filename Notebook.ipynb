{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expresso Customer Churn Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "### 1.1 Business Understanding / Project Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per [Paddle](https://www.paddle.com/resources/customer-attrition#:~:text=Customer%20attrition%20is%20defined%20as,of%20business%20health%20over%20time.), customer churn may be defined as the loss of customers by a business. Despite being a normal part of the customer cycle, it is viewed as a key indicator of business health over time and must be managed to ensure some stability in the business' survival, (retention) strategy development, and/or growth. \n",
    "\n",
    "It is also known as customer attrition or customer turnover, and is calculated as the percentage of customers that stopped using a company's product or service within a specified timeframe. To better manage customer churn, companies should be able to predict it with reasonable accuracy, and that is where machine learning comes in.\n",
    "\n",
    "This project is focused on Vodafone - a telecommunications company - and  aims to predict the likelihood that a customer will churn by identifying and modelling based on the key indicators of churn. Possible strategies that may be explored and implemented to improve retention (or reduce churn) may be recommended in this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains information about the location of clients, the services that they use, the regularity of service use, and their churn status. The columns in the dataset are described below:\n",
    "\n",
    "- **user_id**: user ID\n",
    "- **REGION**: the location of each client\n",
    "- **TENURE**: duration in the network\n",
    "- **MONTANT**: top-up amount\n",
    "- **FREQUENCE_RECH**: number of times the client recharged\n",
    "- **REVENUE**: monthly income of each client\n",
    "- **ARPU_SEGMENT**: income over 90 days / 3\n",
    "- **FREQUENCE**: number of times the client has made an income\n",
    "- **DATA_VOLUME**: number of connections\n",
    "- **ON_NET**: inter expresso call\n",
    "- **ORANGE**: calls to orange\n",
    "- **TIGO**: calls to Tigo\n",
    "- **ZONE1**: calls to zones1\n",
    "- **ZONE2**: calls to zones2\n",
    "- **MRG**: a client who is going\n",
    "- **REGULARITY**: number of times the client is active for 90 days\n",
    "- **TOP_PACK**: the most active packs\n",
    "- **FREQ_TOP_PACK**: number of times the client has activated the top pack packages\n",
    "- **CHURN**: variable to predict - Target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Hypotheses and Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Customers with partners & dependents churn less\n",
    "2. What is the distribution of customers by senior citizenship and how do they churn?\n",
    "3. In terms of tenure, which range of users have churned least?\n",
    "4. At what tenure levels do we lose most customers?\n",
    "5. Customers who exceed the average tenure are less likely to churn\n",
    "6. Users who don't use phone service churn more than phone service users\n",
    "7. Does the use of multiple lines lead to reduced churn?\n",
    "8. DSL users churn more than fibre-optic users\n",
    "9. Users who stream both TV & movies churn less than those who stream only one\n",
    "10. Customers with tech support churn less"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Toolbox Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # Hiding the warnings\n",
    "\n",
    "# Feature Engineering\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "\n",
    "# Models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from xgboost import *\n",
    "\n",
    "# Model evaluation\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import *\n",
    "\n",
    "print(\"Loading complete.\", \"Warnings hidden.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the restriction on columns to display\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "dataset = pd.read_csv(\"Train.csv\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at information about the columns\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast all column names to lowercase\n",
    "dataset.columns = dataset.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for duplicates\n",
    "dataset[dataset.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the dataset preview and the info above, we make the following observations:\n",
    "- Out of the 18 columns, only 5 have no missing values. The columns therefore have to be assessed, and necessary action taken on the columns to deal with the missing values.\n",
    "- There are no duplicates in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Performing initial cleaning on the dataset\n",
    "# dataset[\"TotalCharges\"] = dataset[\"TotalCharges\"].replace(\" \", np.nan)  # replacing the empty spaces in the column with nulls\n",
    "# dataset[\"TotalCharges\"] = pd.to_numeric(dataset[\"TotalCharges\"])  # converting the column to a float\n",
    "\n",
    "# # converting the values to Yes or No\n",
    "# dataset[\"SeniorCitizen\"] = np.where(dataset[\"SeniorCitizen\"] == 0, \"No\", \"Yes\")\n",
    "\n",
    "# # dropping the null values in the dataset\n",
    "# dataset.dropna(inplace= True)\n",
    "# dataset.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# # Dropping the customer ID column\n",
    "# dataset.drop(columns=[\"customerID\"], inplace=True)\n",
    "# dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Exploration of Numeric Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What is the distribution of the columns with numeric values? Are there any outliers?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Looking at the descriptive statistics of the columns with numeric values\n",
    "numerics = [column for column in dataset.columns if (dataset[column].dtype != \"O\") & (len(dataset[column].unique()) > 2)]\n",
    "print(\"Summary table of the Descriptive Statistics of Columns with Numeric Values\")\n",
    "dataset[numerics].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualizing the distributions of the columns with numeric values\n",
    "for column in dataset[numerics].columns:\n",
    "    if len(dataset[column].unique()) > 2:\n",
    "\n",
    "        # Visualizing the distribution of categories inside the column\n",
    "        fig = px.box(dataset[numerics], y=column, labels={\"color\": \"Churned\",\n",
    "                                                          \"tenure\": \"Tenure (months)\",\n",
    "                                                          },\n",
    "                     title=f\"A visual representation of values in the {column} column\"\n",
    "                     )\n",
    "        fig.show()\n",
    "\n",
    "        # Visualizing the proportion of churn for each category inside the column\n",
    "        fig = px.box(dataset[numerics], y=column, color=dataset[\"churn\"], labels={\"color\": \"Churned\",\n",
    "                                                                                  \"tenure\": \"Tenure (months)\",\n",
    "                                                                                  },\n",
    "                     title=f\"A visual representation of values in the {column} column split by churn levels\"\n",
    "                     )\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Exploration of Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualizing the distribution of the columns with categorical values and their churn levels\n",
    "categoricals = [column for column in dataset.columns if (dataset[column].dtype == \"O\")]\n",
    "\n",
    "for column in dataset[categoricals].columns:\n",
    "    # Visualizing the distribution of the categories in the column\n",
    "    fig = px.histogram(dataset, x=dataset[column], text_auto=True,\n",
    "                       title=f\"Distribution of values in the {column} column\")\n",
    "    fig.show()\n",
    "\n",
    "    # Visualizing the churn proportions of the categories in the column\n",
    "    fig = px.histogram(dataset, x=dataset[column], color=\"churn\", barnorm=\"percent\", text_auto=\".2f\",\n",
    "                       title=f\"Churn proportions of users in {column} column\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Answering the other questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Feature Engineering\n",
    "### 5.1 Feature Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the unique values in each column\n",
    "dataset.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preview**\n",
    "\n",
    "Here, the columns with two unique values (yes or no) will be encoded using label encoding, where 1 is assigned to yes and 0 assigned to no.\n",
    "\n",
    "For gender, since there is no ordinal arrangement between the two categories (male and female), it will be encoded using one-hot encoding.\n",
    "\n",
    "The rest of the categorical columns will also be encoded using one-hot encoding.\n",
    "\n",
    "The first columns will be dropped for all the columns that are encoded using one-hot encoding.\n",
    "\n",
    "The numeric columns (tenure, monthly charges, and total charges) will be scaled using the MinMaxScaler to ensure that their structure/distributions are preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradio_set = dataset.drop(columns= [\"Churn\"])\n",
    "gradio_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the dataset for use in the Gradio app\n",
    "gradio_set.to_csv(\"churn_prediction_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the churn column\n",
    "dataset[\"Churn\"].replace({\"Yes\":1, \"No\":0}, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of categoricals\n",
    "categoricals.remove(\"Churn\")\n",
    "categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the categorical variables\n",
    "oh_encoder = OneHotEncoder(drop = \"first\", sparse = False)\n",
    "oh_encoder.fit(dataset[categoricals])\n",
    "encoded_categoricals = oh_encoder.transform(dataset[categoricals])\n",
    "encoded_categoricals = pd.DataFrame(encoded_categoricals, columns = oh_encoder.get_feature_names_out().tolist())\n",
    "encoded_categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the encoded categoricals to the DataFrame and dropping the original columns\n",
    "complete_set = dataset.join(encoded_categoricals)\n",
    "complete_set.drop(columns= categoricals, inplace= True)\n",
    "complete_set.rename(columns= lambda x: re.sub(\"[^A-Za-z0-9_]+\", \"\", x), inplace= True)\n",
    "complete_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profiling the final dataframe with SweetViz\n",
    "final_df_report = sv.analyze(complete_set)\n",
    "final_df_report.show_html(filepath=\"final_df_report.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1 Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Looking at the correlation between the variables in the merged dataframe\n",
    "correlation = pd.DataFrame(complete_set.corr())\n",
    "\n",
    "# Defining a colourscale for the correlation plot\n",
    "colorscale = [[0.0, \"rgb(255,255,255)\"], [0.2, \"rgb(255, 255, 153)\"],\n",
    "              [0.4, \"rgb(153, 255, 204)\"], [0.6, \"rgb(179, 217, 255)\"],\n",
    "              [0.8, \"rgb(240, 179, 255)\"], [1.0, \"rgb(255, 77, 148)\"]\n",
    "              ]\n",
    "\n",
    "# Plotting the Correlation Matrix\n",
    "fig = px.imshow(correlation,\n",
    "                text_auto=\".3f\",\n",
    "                aspect=\"auto\",\n",
    "                labels={\"color\": \"Correlation Coefficient\"},\n",
    "                contrast_rescaling=\"minmax\",\n",
    "                color_continuous_scale=colorscale\n",
    "                )\n",
    "fig.update_xaxes(side=\"top\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation matrix presents a more comprehensive view on the nature of the relationships between the various variables  in the dataset, but it is not so clear due to the number of features. As such, other methods will be used to explore the features and their potential importances for the modelling process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2 Feature Selection using SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the target & predictor variables\n",
    "X = complete_set.drop(columns=[\"Churn\"])\n",
    "y = complete_set[\"Churn\"]\n",
    "\n",
    "# Splitting the dataframe into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state= 24, stratify= y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fitting the variables to the function\n",
    "best_features = SelectKBest(score_func=chi2, k=\"all\")\n",
    "fit = best_features.fit(X_train, y_train)\n",
    "\n",
    "# Looking at the features & their importances\n",
    "feature_scores = pd.DataFrame(fit.scores_)\n",
    "selected_columns = pd.DataFrame(X_train.columns)\n",
    "columns_x_scores = pd.concat([selected_columns, feature_scores], axis=1)\n",
    "columns_x_scores.columns = [\"Feature\", \"Score\"]\n",
    "\n",
    "# print 10 largest scores & features\n",
    "print(columns_x_scores.nlargest(10, \"Score\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# Visualizing the top 10 most important features\n",
    "fig = px.bar(columns_x_scores.nlargest(10, \"Score\"), x=\"Feature\", y=\"Score\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot and table above, we note that the top 5 most important features for churn prediction are total charges (by a mile), tenure, monthly charges, 2-year contracts, and electronic check payment method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.3 Feature Importance using Extra Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the Classifier and fitting it to the training data\n",
    "etc_model = ExtraTreesClassifier()\n",
    "etc_model.fit(X_train, y_train)\n",
    "print(etc_model.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe of the features and their importances for plotting\n",
    "feature_importance_lvls = pd.DataFrame(etc_model.feature_importances_, index= X_train.columns).reset_index()\n",
    "feature_importance_lvls.rename(columns= {\"index\": \"Feature\", 0: \"Importance\"},inplace= True)\n",
    "feature_importance_lvls.sort_values(by= \"Importance\", ascending= False, inplace= True)\n",
    "feature_importance_lvls.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the top 20 most important features\n",
    "fig = px.bar(feature_importance_lvls[:20], x=\"Feature\", y=\"Importance\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going by the results of the ExtraTreesClassifier (above), the top 5 most important features for churn prediction are tenure, total charges, monthly charges, electronic check payment method, and fibre optic internet service. This matches, to a large extent, the results from the SelectKBest model which also suggested 4 of the top 5 features here.\n",
    "\n",
    "For now, no features will be removed before the modelling process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.0 Modelling*\n",
    "**With regard to the imbalance in the dataset*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preview**\n",
    "\n",
    "1. Train_test_split\n",
    "    \n",
    "For the modelling, the already defined train data (from Section 5.2.2) will be split again into training and testing so that the models that will be built will be cross-validated and evaluated based on them.\n",
    "\n",
    "The test data (from Section 5.2.2) above will be the holdout sample. It is on this data that the best model(s) will make their predictions and have their final evaluation.\n",
    "\n",
    "To do this, the X_train and y_train will be put together as \"train_data\" before being split into the train and test samples.\n",
    "The X_test and y_test will also be put together as \"test_data\" but will not be split until it is time for prediction and final evaluation of the best model(s).\n",
    "\n",
    "    \n",
    "2. Dataset Balancing\n",
    "\n",
    "Given that the dataset is imbalanced, it would have to be balanced before modelling to reduce the error in prediction since our target is the minority class. The 3 most common methods for balancing are oversampling, undersampling, and SMOTE. Since this project doubles as a study opportunity, I will apply each method to (copies of) the training data and build models under each. The models will then be evaluated before selecting the best one(s) for optimization and application on the final test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting the training dataset together for modelling\n",
    "train_data = X_train.join(y_train, on=X_train.index)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting the training dataset together for modelling\n",
    "test_data = X_test.join(y_test, on=X_test.index)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a copy of the training data and checking the shape\n",
    "oversampling_data = train_data.copy()\n",
    "count_not_churned, count_churned = oversampling_data[\"Churn\"].value_counts()\n",
    "count_not_churned, count_churned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the dataframe for observations for the various classes\n",
    "not_churned = oversampling_data[oversampling_data[\"Churn\"] == 0]\n",
    "churned = oversampling_data[oversampling_data[\"Churn\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversampling the churned class and combining the \"balanced\" DataFrame\n",
    "churn_oversampled = churned.sample(count_not_churned, replace=True)\n",
    "df_oversampled = pd.concat([not_churned, churn_oversampled])\n",
    "\n",
    "print(\"Random over-sampling:\")\n",
    "print(df_oversampled[\"Churn\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the target & predictor variables\n",
    "os_X = df_oversampled.drop(columns=[\"Churn\"])\n",
    "os_y = df_oversampled[\"Churn\"]\n",
    "\n",
    "# Splitting the dataframe\n",
    "os_X_train, os_X_test, os_y_train, os_y_test = train_test_split(os_X, os_y, test_size=0.25, random_state=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the numeric columns\n",
    "columns_to_scale = [\"tenure\", \"MonthlyCharges\", \"TotalCharges\"]\n",
    "\n",
    "os_scaler = MinMaxScaler()\n",
    "\n",
    "os_X_train[columns_to_scale] = os_scaler.fit_transform(os_X_train[columns_to_scale])\n",
    "os_X_test[columns_to_scale] = os_scaler.transform(os_X_test[columns_to_scale])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "log_reg = LogisticRegression(random_state=24)\n",
    "os_log_reg_model = log_reg.fit(os_X_train, os_y_train)\n",
    "\n",
    "# Feature Importance of the Random Forest Model\n",
    "log_reg_importance = os_log_reg_model.coef_[0]\n",
    "log_reg_importance = pd.DataFrame(log_reg_importance, index=os_X.columns)\n",
    "log_reg_importance.reset_index(inplace=True)\n",
    "log_reg_importance.rename(columns={\n",
    "    \"index\": \"Feature\",\n",
    "    0: \"Score\"\n",
    "}, inplace=True)\n",
    "log_reg_importance.sort_values(by=\"Score\", ascending=False, inplace=True)\n",
    "log_reg_importance\n",
    "\n",
    "# Visualizing the feature importances\n",
    "fig = px.bar(log_reg_importance, x=\"Feature\", y=\"Score\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "oversampled_log_reg_pred = os_log_reg_model.predict(os_X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "oversampled_log_reg_report = classification_report(os_y_test, oversampled_log_reg_pred, target_names=[\"Stayed\", \"Churned\"])\n",
    "print(oversampled_log_reg_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the F2 Score\n",
    "f2_score = fbeta_score(os_y_test, oversampled_log_reg_pred, beta=0.5)\n",
    "f2_score = \"{:.5f}\".format(f2_score)\n",
    "print(\"F2 Score:\", f2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Confusion Matrix\n",
    "oslr_conf_mat = confusion_matrix(os_y_test, oversampled_log_reg_pred)\n",
    "oslr_conf_mat = pd.DataFrame(oslr_conf_mat).reset_index(drop=True)\n",
    "oslr_conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the Confusion Matrix\n",
    "f, ax = plt.subplots()\n",
    "sns.heatmap(oslr_conf_mat,\n",
    "            annot=True,\n",
    "            linewidth=1.0,\n",
    "            fmt=\".0f\",\n",
    "            cmap=\"RdPu\",\n",
    "            ax=ax)\n",
    "plt.xlabel = (\"y_pred\")\n",
    "plt.ylabel = (\"y_true\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.2 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the model\n",
    "dt_clf = DecisionTreeClassifier(random_state=24)\n",
    "os_dt_model = dt_clf.fit(os_X_train, os_y_train)\n",
    "\n",
    "# Feature importances\n",
    "dt_importance = os_dt_model.feature_importances_\n",
    "dt_importance = pd.DataFrame(dt_importance, columns=[\"score\"]).reset_index()\n",
    "dt_importance[\"Feature\"] = list(os_X.columns)\n",
    "dt_importance.drop(columns=[\"index\"], inplace=True)\n",
    "\n",
    "dt_importance.sort_values(by=\"score\",\n",
    "                          ascending=False,\n",
    "                          ignore_index=True,\n",
    "                          inplace=True)\n",
    "\n",
    "# Plotting the feature importances\n",
    "fig = px.bar(dt_importance, x=\"Feature\", y=\"score\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "oversampled_dt_pred = os_dt_model.predict(os_X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "oversampled_dt_report = classification_report(os_y_test, oversampled_dt_pred)\n",
    "print(oversampled_dt_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the F2 Score\n",
    "f2_score = fbeta_score(os_y_test, oversampled_dt_pred, beta=0.5)\n",
    "f2_score = \"{:.5f}\".format(f2_score)\n",
    "print(\"F2 Score:\", f2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Confusion Matrix\n",
    "os_dt_conf_mat = confusion_matrix(os_y_test, oversampled_dt_pred)\n",
    "os_dt_conf_mat = pd.DataFrame(os_dt_conf_mat).reset_index(drop=True)\n",
    "os_dt_conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the Confusion Matrix\n",
    "f, ax = plt.subplots()\n",
    "sns.heatmap(os_dt_conf_mat,\n",
    "            annot=True,\n",
    "            linewidth=1.0,\n",
    "            fmt=\".0f\",\n",
    "            cmap=\"RdPu\",\n",
    "            ax=ax)\n",
    "plt.xlabel = (\"y_pred\")\n",
    "plt.ylabel = (\"y_true\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.3 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forests\n",
    "rf_clf = RandomForestClassifier(random_state=24)\n",
    "os_rf_model = rf_clf.fit(os_X_train, os_y_train)\n",
    "\n",
    "# Feature Importance of the Random Forest Model\n",
    "rf_importance = os_rf_model.feature_importances_\n",
    "rf_importance = pd.DataFrame(rf_importance, columns=[\"score\"]).reset_index()\n",
    "rf_importance[\"Feature\"] = list(os_X.columns)\n",
    "rf_importance.drop(columns=[\"index\"], inplace=True)\n",
    "rf_importance.sort_values(by=\"score\",\n",
    "                          ascending=False,\n",
    "                          ignore_index=True,\n",
    "                          inplace=True)\n",
    "\n",
    "# Visualizing the feature importances\n",
    "fig = px.bar(rf_importance, x=\"Feature\", y=\"score\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "oversampled_rf_pred = os_rf_model.predict(os_X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "oversampled_rf_report = classification_report(os_y_test, oversampled_rf_pred)\n",
    "print(oversampled_rf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the F2 Score\n",
    "f2_score = fbeta_score(os_y_test, oversampled_rf_pred, beta=0.5)\n",
    "f2_score = \"{:.5f}\".format(f2_score)\n",
    "print(\"F2 Score:\", f2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Confusion Matrix\n",
    "os_rf_conf_mat = confusion_matrix(os_y_test, oversampled_rf_pred)\n",
    "os_rf_conf_mat = pd.DataFrame(os_rf_conf_mat).reset_index(drop=True)\n",
    "os_rf_conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the Confusion Matrix\n",
    "f, ax = plt.subplots()\n",
    "sns.heatmap(os_rf_conf_mat,\n",
    "            annot=True,\n",
    "            linewidth=1.0,\n",
    "            fmt=\".0f\",\n",
    "            cmap=\"RdPu\",\n",
    "            ax=ax)\n",
    "plt.xlabel = (\"y_pred\")\n",
    "plt.ylabel = (\"y_true\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.4 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting model to the training data\n",
    "xgb_clf = XGBClassifier(random_state=24)\n",
    "os_xgb_model = xgb_clf.fit(os_X_train, os_y_train)\n",
    "\n",
    "# Feature Importance of the Random Forest Model\n",
    "xgb_importance = os_xgb_model.feature_importances_\n",
    "xgb_importance = pd.DataFrame(xgb_importance, columns=[\"score\"]).reset_index()\n",
    "xgb_importance[\"Feature\"] = list(os_X.columns)\n",
    "xgb_importance.drop(columns=[\"index\"], inplace=True)\n",
    "xgb_importance.sort_values(by=\"score\",\n",
    "                           ascending=False,\n",
    "                           ignore_index=True,\n",
    "                           inplace=True)\n",
    "\n",
    "# Visualizing the feature importances\n",
    "fig = px.bar(xgb_importance, x=\"Feature\", y=\"score\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "oversampled_xgb_pred = os_xgb_model.predict(os_X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "oversampled_xgb_report = classification_report(os_y_test, oversampled_xgb_pred)\n",
    "print(oversampled_xgb_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the F2 Score\n",
    "f2_score = fbeta_score(os_y_test, oversampled_xgb_pred, beta=0.5)\n",
    "f2_score = \"{:.5f}\".format(f2_score)\n",
    "print(\"F2 Score:\", f2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Confusion Matrix\n",
    "os_xgb_conf_mat = confusion_matrix(os_y_test, oversampled_xgb_pred)\n",
    "os_xgb_conf_mat = pd.DataFrame(os_xgb_conf_mat).reset_index(drop=True)\n",
    "os_xgb_conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the Confusion Matrix\n",
    "f, ax = plt.subplots()\n",
    "sns.heatmap(os_xgb_conf_mat,\n",
    "            annot=True,\n",
    "            linewidth=1.0,\n",
    "            fmt=\".0f\",\n",
    "            cmap=\"RdPu\",\n",
    "            ax=ax)\n",
    "plt.xlabel = (\"y_pred\")\n",
    "plt.ylabel = (\"y_true\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.5 CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initializing CatBoostClassifier\n",
    "oversampled_catb_clf = CatBoostClassifier(metric_period=100, random_state=24)\n",
    "\n",
    "# Fitting it to the training data\n",
    "oversampled_catb_model = oversampled_catb_clf.fit(os_X_train, os_y_train)\n",
    "\n",
    "# Feature Importance of the Model\n",
    "oversampled_catb_importance = oversampled_catb_model.feature_importances_\n",
    "\n",
    "oversampled_catb_importance = pd.DataFrame(oversampled_catb_importance,\n",
    "                                           columns=[\"score\"]).reset_index()\n",
    "\n",
    "oversampled_catb_importance[\"Feature\"] = list(os_X.columns)\n",
    "\n",
    "oversampled_catb_importance.drop(columns=[\"index\"], inplace=True)\n",
    "\n",
    "oversampled_catb_importance.sort_values(by=\"score\",\n",
    "                                        ascending=False,\n",
    "                                        ignore_index=True,\n",
    "                                        inplace=True)\n",
    "\n",
    "fig = px.bar(oversampled_catb_importance, x=\"Feature\", y=\"score\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Making the predictions\n",
    "oversampled_catb_pred = oversampled_catb_model.predict(os_X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "oversampled_catb_report = classification_report(os_y_test,\n",
    "                                                oversampled_catb_pred,\n",
    "                                                target_names=[\"Stayed\", \"Churned\"])\n",
    "print(oversampled_catb_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the F2 Score\n",
    "f2_score = fbeta_score(os_y_test, oversampled_catb_pred, beta=0.5)\n",
    "f2_score = \"{:.5f}\".format(f2_score)\n",
    "print(\"F2 Score:\", f2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Confusion Matrix\n",
    "os_catb_conf_mat = confusion_matrix(os_y_test, oversampled_catb_pred)\n",
    "os_catb_conf_mat = pd.DataFrame(os_catb_conf_mat).reset_index(drop=True)\n",
    "os_catb_conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the Confusion Matrix\n",
    "f, ax = plt.subplots()\n",
    "sns.heatmap(os_catb_conf_mat,\n",
    "            annot=True,\n",
    "            linewidth=1.0,\n",
    "            fmt=\".0f\",\n",
    "            cmap=\"RdPu\",\n",
    "            ax=ax\n",
    "            )\n",
    "plt.xlabel = (\"y_pred\")\n",
    "plt.ylabel = (\"y_true\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.6 LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing LightGBM Classifier\n",
    "oversampled_lgb_clf = lgb.LGBMClassifier(random_state=24)\n",
    "\n",
    "# Fitting it to the training data\n",
    "oversampled_lgb_model = oversampled_lgb_clf.fit(os_X_train, os_y_train)\n",
    "\n",
    "# Feature Importance of the Model\n",
    "oversampled_lgb_importance = oversampled_lgb_model.feature_importances_\n",
    "\n",
    "oversampled_lgb_importance = pd.DataFrame(oversampled_lgb_importance,\n",
    "                                          columns=[\"score\"]).reset_index()\n",
    "\n",
    "oversampled_lgb_importance[\"Feature\"] = list(os_X.columns)\n",
    "oversampled_lgb_importance.drop(columns=[\"index\"], inplace=True)\n",
    "oversampled_lgb_importance.sort_values(by=\"score\",\n",
    "                                       ascending=False,\n",
    "                                       ignore_index=True,\n",
    "                                       inplace=True\n",
    "                                       )\n",
    "\n",
    "fig = px.bar(oversampled_lgb_importance, x=\"Feature\", y=\"score\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the predictions\n",
    "oversampled_lgb_pred = oversampled_lgb_model.predict(os_X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "oversampled_lgb_report = classification_report(os_y_test, oversampled_lgb_pred, target_names=[\"Stayed\", \"Churned\"])\n",
    "print(oversampled_lgb_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the F2 Score\n",
    "f2_score = fbeta_score(os_y_test, oversampled_lgb_pred, beta=0.5)\n",
    "f2_score = \"{:.5f}\".format(f2_score)\n",
    "print(\"F2 Score:\", f2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Confusion Matrix\n",
    "os_lgb_conf_mat = confusion_matrix(os_y_test, oversampled_lgb_pred)\n",
    "os_lgb_conf_mat = pd.DataFrame(os_lgb_conf_mat).reset_index(drop=True)\n",
    "os_lgb_conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the Confusion Matrix\n",
    "f, ax = plt.subplots()\n",
    "sns.heatmap(os_lgb_conf_mat,\n",
    "            annot=True,\n",
    "            linewidth=1.0,\n",
    "            fmt=\".0f\",\n",
    "            cmap=\"RdPu\",\n",
    "            ax=ax)\n",
    "plt.xlabel = (\"y_pred\")\n",
    "plt.ylabel = (\"y_true\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampling_models = {\n",
    "    \"os_lr\": os_log_reg_model,\n",
    "    \"os_dt\": os_dt_model,\n",
    "    \"os_rf\": os_rf_model,\n",
    "    \"os_xgb\": os_xgb_model,\n",
    "    \"os_catb\": oversampled_catb_model,\n",
    "    \"os_lgb\": oversampled_lgb_model\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.7 Summarizing the Performance of the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a helper function to evaluate the models at a go\n",
    "def evaluation(fit_models, X_test, y_test):\n",
    "    lst = []\n",
    "    for name, model in fit_models.items():\n",
    "        pred = model.predict(X_test)\n",
    "\n",
    "        f2_score = fbeta_score(y_test, pred, beta=0.5)\n",
    "        f2_score = \"{:.5f}\".format(f2_score)\n",
    "\n",
    "        lst.append([\n",
    "            name,\n",
    "            precision_score(y_test, pred, average=\"weighted\"),\n",
    "            recall_score(y_test, pred, average=\"weighted\"),\n",
    "            f1_score(y_test, pred, average=\"weighted\"),\n",
    "            accuracy_score(y_test, pred),\n",
    "            f2_score\n",
    "        ])\n",
    "\n",
    "    eval_df = pd.DataFrame(lst, columns=[\"model\", \"precision\", \"recall\", \"f1_weighted\", \"accuracy\", \"f2_score\"])\n",
    "    eval_df.set_index(\"model\", inplace=True)\n",
    "    return eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampled_models_eval = evaluation(oversampling_models, os_X_test, os_y_test)\n",
    "oversampled_models_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One more look at the training data\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersampling the churned class and combining the \"balanced\" DataFrame\n",
    "churn_undersampled = not_churned.sample(count_churned)\n",
    "df_undersampled = pd.concat([churn_undersampled, churned])\n",
    "\n",
    "print('Random under-sampling:')\n",
    "print(df_undersampled[\"Churn\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the target & predictor variables\n",
    "X = df_undersampled.drop(columns=[\"Churn\"])\n",
    "y = df_undersampled[\"Churn\"]\n",
    "\n",
    "# Splitting the dataframe\n",
    "us_X_train, us_X_test, us_y_train, us_y_test = train_test_split(X, y, test_size=0.25, random_state=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the numeric columns\n",
    "us_scaler = MinMaxScaler()\n",
    "us_X_train[columns_to_scale] = us_scaler.fit_transform(us_X_train[columns_to_scale])\n",
    "us_X_test[columns_to_scale] = us_scaler.transform(us_X_test[columns_to_scale])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "log_reg = LogisticRegression()\n",
    "us_log_reg_model = log_reg.fit(us_X_train, us_y_train)\n",
    "\n",
    "# Feature Importance of the Random Forest Model\n",
    "us_log_reg_importance = us_log_reg_model.coef_[0]\n",
    "us_log_reg_importance = pd.DataFrame(us_log_reg_importance, index=X.columns)\n",
    "us_log_reg_importance.reset_index(inplace=True)\n",
    "us_log_reg_importance.rename(columns={\n",
    "    \"index\": \"Feature\",\n",
    "    0: \"Score\"\n",
    "},\n",
    "    inplace=True)\n",
    "us_log_reg_importance.sort_values(by=\"Score\", ascending=False, inplace=True)\n",
    "us_log_reg_importance\n",
    "\n",
    "fig = px.bar(us_log_reg_importance, x=\"Feature\", y=\"Score\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "us_log_reg_pred = us_log_reg_model.predict(us_X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "us_log_reg_report = classification_report(us_y_test, us_log_reg_pred)\n",
    "print(us_log_reg_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the F2 Score\n",
    "f2_score = fbeta_score(us_y_test, us_log_reg_pred, beta=0.5)\n",
    "f2_score = \"{:.5f}\".format(f2_score)\n",
    "print(\"F2 Score:\", f2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Confusion Matrix\n",
    "us_lr_conf_mat = confusion_matrix(us_y_test, us_log_reg_pred)\n",
    "us_lr_conf_mat = pd.DataFrame(us_lr_conf_mat).reset_index(drop=True)\n",
    "us_lr_conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the Confusion Matrix\n",
    "f, ax = plt.subplots()\n",
    "sns.heatmap(us_lr_conf_mat,\n",
    "            annot=True,\n",
    "            linewidth=1.0,\n",
    "            fmt=\".0f\",\n",
    "            cmap=\"RdPu\",\n",
    "            ax=ax)\n",
    "plt.xlabel = (\"y_pred\")\n",
    "plt.ylabel = (\"y_true\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.2 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "dt_clf = DecisionTreeClassifier(random_state=24)\n",
    "us_dt_model = dt_clf.fit(us_X_train, us_y_train)\n",
    "\n",
    "# Feature importances\n",
    "us_dt_importance = us_dt_model.feature_importances_\n",
    "us_dt_importance = pd.DataFrame(us_dt_importance,\n",
    "                                columns=[\"score\"]).reset_index()\n",
    "us_dt_importance[\"Feature\"] = list(X.columns)\n",
    "us_dt_importance.drop(columns=[\"index\"], inplace=True)\n",
    "us_dt_importance.sort_values(by=\"score\",\n",
    "                             ascending=False,\n",
    "                             ignore_index=True,\n",
    "                             inplace=True)\n",
    "\n",
    "# Plotting the feature importances\n",
    "fig = px.bar(us_dt_importance, x=\"Feature\", y=\"score\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "us_dt_pred = us_dt_model.predict(us_X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "us_dt_report = classification_report(us_y_test, us_dt_pred)\n",
    "print(us_dt_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the F2 Score\n",
    "f2_score = fbeta_score(us_y_test, us_dt_pred, beta=0.5)\n",
    "f2_score = \"{:.5f}\".format(f2_score)\n",
    "print(\"F2 Score:\", f2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Confusion Matrix\n",
    "us_dt_conf_mat = confusion_matrix(us_y_test, us_dt_pred)\n",
    "us_dt_conf_mat = pd.DataFrame(us_dt_conf_mat).reset_index(drop=True)\n",
    "us_dt_conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the Confusion Matrix\n",
    "f, ax = plt.subplots()\n",
    "sns.heatmap(us_dt_conf_mat,\n",
    "            annot=True,\n",
    "            linewidth=1.0,\n",
    "            fmt=\".0f\",\n",
    "            cmap=\"RdPu\",\n",
    "            ax=ax)\n",
    "plt.xlabel = (\"y_pred\")\n",
    "plt.ylabel = (\"y_true\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.3 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# Random Forests\n",
    "rf_clf = RandomForestClassifier(random_state=24)\n",
    "us_rf_model = rf_clf.fit(us_X_train, us_y_train)\n",
    "\n",
    "# Feature Importance of the Random Forest Model\n",
    "us_rf_importance = us_rf_model.feature_importances_\n",
    "us_rf_importance = pd.DataFrame(us_rf_importance,\n",
    "                                columns=[\"score\"]).reset_index()\n",
    "\n",
    "us_rf_importance[\"Feature\"] = list(X.columns)\n",
    "us_rf_importance.drop(columns=[\"index\"], inplace=True)\n",
    "\n",
    "us_rf_importance.sort_values(by=\"score\",\n",
    "                             ascending=False,\n",
    "                             ignore_index=True,\n",
    "                             inplace=True)\n",
    "\n",
    "fig = px.bar(us_rf_importance, x=\"Feature\", y=\"score\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "us_rf_pred = us_rf_model.predict(us_X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "us_rf_report = classification_report(us_y_test, us_rf_pred)\n",
    "print(us_rf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the F2 Score\n",
    "f2_score = fbeta_score(us_y_test, us_rf_pred, beta=0.5)\n",
    "f2_score = \"{:.5f}\".format(f2_score)\n",
    "print(\"F2 Score:\", f2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Confusion Matrix\n",
    "us_rf_conf_mat = confusion_matrix(us_y_test, us_rf_pred)\n",
    "us_rf_conf_mat = pd.DataFrame(us_rf_conf_mat).reset_index(drop=True)\n",
    "us_rf_conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the Confusion Matrix\n",
    "f, ax = plt.subplots()\n",
    "sns.heatmap(us_rf_conf_mat,\n",
    "            annot=True,\n",
    "            linewidth=1.0,\n",
    "            fmt=\".0f\",\n",
    "            cmap=\"RdPu\",\n",
    "            ax=ax)\n",
    "plt.xlabel = (\"y_pred\")\n",
    "plt.ylabel = (\"y_true\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.4 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting model to the training data\n",
    "xgb_clf = XGBClassifier(random_state=24)\n",
    "us_xgb_model = xgb_clf.fit(us_X_train, us_y_train)\n",
    "\n",
    "# Feature Importance of the Random Forest Model\n",
    "us_xgb_importance = us_xgb_model.feature_importances_\n",
    "us_xgb_importance = pd.DataFrame(us_xgb_importance,\n",
    "                                 columns=[\"score\"]).reset_index()\n",
    "\n",
    "us_xgb_importance[\"Feature\"] = list(X.columns)\n",
    "\n",
    "us_xgb_importance.drop(columns=[\"index\"], inplace=True)\n",
    "\n",
    "us_xgb_importance.sort_values(by=\"score\",\n",
    "                              ascending=False,\n",
    "                              ignore_index=True,\n",
    "                              inplace=True)\n",
    "\n",
    "fig = px.bar(us_xgb_importance, x=\"Feature\", y=\"score\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the predictions\n",
    "us_xgb_pred = us_xgb_model.predict(us_X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "us_xgb_report = classification_report(us_y_test, us_xgb_pred)\n",
    "print(us_xgb_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the F2 Score\n",
    "f2_score = fbeta_score(us_y_test, us_xgb_pred, beta=0.5)\n",
    "f2_score = \"{:.5f}\".format(f2_score)\n",
    "print(\"F2 Score:\", f2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Confusion Matrix\n",
    "us_xgb_conf_mat = confusion_matrix(us_y_test, us_xgb_pred)\n",
    "us_xgb_conf_mat = pd.DataFrame(us_xgb_conf_mat).reset_index(drop=True)\n",
    "us_xgb_conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the Confusion Matrix\n",
    "f, ax = plt.subplots()\n",
    "sns.heatmap(us_xgb_conf_mat,\n",
    "            annot=True,\n",
    "            linewidth=1.0,\n",
    "            fmt=\".0f\",\n",
    "            cmap=\"RdPu\",\n",
    "            ax=ax)\n",
    "plt.xlabel = (\"y_pred\")\n",
    "plt.ylabel = (\"y_true\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.5 CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing CatBoostClassifier\n",
    "us_catb_clf = CatBoostClassifier(metric_period=100, random_state=24)\n",
    "\n",
    "# Fitting it to the training data\n",
    "us_catb_model = us_catb_clf.fit(us_X_train, us_y_train)\n",
    "\n",
    "# Feature Importance of the Model\n",
    "us_catb_importance = us_catb_model.feature_importances_\n",
    "us_catb_importance = pd.DataFrame(us_catb_importance,\n",
    "                                  columns=[\"score\"]).reset_index()\n",
    "us_catb_importance[\"Feature\"] = list(X.columns)\n",
    "us_catb_importance.drop(columns=[\"index\"], inplace=True)\n",
    "us_catb_importance.sort_values(by=\"score\",\n",
    "                               ascending=False,\n",
    "                               ignore_index=True,\n",
    "                               inplace=True)\n",
    "\n",
    "fig = px.bar(us_catb_importance, x=\"Feature\", y=\"score\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the predictions\n",
    "us_catb_pred = us_catb_model.predict(us_X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "us_catb_report = classification_report(\n",
    "    us_y_test, us_catb_pred, target_names=[\"Stayed\", \"Churned\"])\n",
    "print(us_catb_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the F2 Score\n",
    "f2_score = fbeta_score(us_y_test, us_catb_pred, beta=0.5)\n",
    "f2_score = \"{:.5f}\".format(f2_score)\n",
    "print(\"F2 Score:\", f2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Confusion Matrix\n",
    "us_catb_conf_mat = confusion_matrix(us_y_test, us_catb_pred)\n",
    "us_catb_conf_mat = pd.DataFrame(us_catb_conf_mat).reset_index(drop=True)\n",
    "us_catb_conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the Confusion Matrix\n",
    "f, ax = plt.subplots()\n",
    "sns.heatmap(us_catb_conf_mat,\n",
    "            annot=True,\n",
    "            linewidth=1.0,\n",
    "            fmt=\".0f\",\n",
    "            cmap=\"RdPu\",\n",
    "            ax=ax)\n",
    "plt.xlabel = (\"y_pred\")\n",
    "plt.ylabel = (\"y_true\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.6 LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing LightGBM Classifier\n",
    "us_lgb_clf = lgb.LGBMClassifier(random_state=24)\n",
    "\n",
    "# Fitting it to the training data\n",
    "us_lgb_model = us_lgb_clf.fit(us_X_train, us_y_train)\n",
    "\n",
    "# Feature Importance of the Model\n",
    "us_lgb_importance = us_lgb_model.feature_importances_\n",
    "us_lgb_importance = pd.DataFrame(us_lgb_importance,\n",
    "                                 columns=[\"score\"]).reset_index()\n",
    "us_lgb_importance[\"Feature\"] = list(X.columns)\n",
    "us_lgb_importance.drop(columns=[\"index\"], inplace=True)\n",
    "us_lgb_importance.sort_values(by=\"score\",\n",
    "                              ascending=False,\n",
    "                              ignore_index=True,\n",
    "                              inplace=True)\n",
    "\n",
    "fig = px.bar(us_lgb_importance, x=\"Feature\", y=\"score\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the predictions\n",
    "us_lgb_pred = us_lgb_model.predict(us_X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "us_lgb_report = classification_report(\n",
    "    us_y_test, us_lgb_pred, target_names=[\"Stayed\", \"Churned\"])\n",
    "print(us_lgb_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the F2 Score\n",
    "f2_score = fbeta_score(us_y_test, us_lgb_pred, beta=0.5)\n",
    "f2_score = \"{:.5f}\".format(f2_score)\n",
    "print(\"F2 Score:\", f2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Confusion Matrix\n",
    "us_lgb_conf_mat = confusion_matrix(us_y_test, us_lgb_pred)\n",
    "us_lgb_conf_mat = pd.DataFrame(us_lgb_conf_mat).reset_index(drop=True)\n",
    "us_lgb_conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the Confusion Matrix\n",
    "f, ax = plt.subplots()\n",
    "sns.heatmap(us_lgb_conf_mat,\n",
    "            annot=True,\n",
    "            linewidth=1.0,\n",
    "            fmt=\".0f\",\n",
    "            cmap=\"RdPu\",\n",
    "            ax=ax)\n",
    "plt.xlabel = (\"y_pred\")\n",
    "plt.ylabel = (\"y_true\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.7 Summarizing the Performance of the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "undersampling_models = {\n",
    "    \"us_lr\": us_log_reg_model,\n",
    "    \"us_dt\": us_dt_model,\n",
    "    \"us_rf\": us_rf_model,\n",
    "    \"us_xgb\": us_xgb_model,\n",
    "    \"us_catb\": us_catb_model,\n",
    "    \"us_lgb\": us_lgb_model\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "undersampled_models_eval = evaluation(\n",
    "    undersampling_models, us_X_test, us_y_test)\n",
    "undersampled_models_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a copy of the the training dataframe for the SMOTE\n",
    "smote_data = train_data.copy()\n",
    "X = smote_data.drop(columns=[\"Churn\"])\n",
    "y = smote_data[\"Churn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Resampling the dataframe using SMOTE\n",
    "smote = SMOTE(sampling_strategy=\"auto\", random_state = 24)\n",
    "X_sm, y_sm = smote.fit_resample(X, y)\n",
    "y_sm.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataframe\n",
    "sm_X_train, sm_X_test, sm_y_train, sm_y_test = train_test_split(X_sm, y_sm, test_size=0.25, random_state=24, stratify=y_sm)\n",
    "sm_y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the numeric columns\n",
    "sm_scaler = MinMaxScaler()\n",
    "\n",
    "sm_X_train[columns_to_scale] = sm_scaler.fit_transform(sm_X_train[columns_to_scale])\n",
    "sm_X_test[columns_to_scale] = sm_scaler.transform(sm_X_test[columns_to_scale])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "log_reg = LogisticRegression(random_state=24)\n",
    "sm_log_reg_model = log_reg.fit(sm_X_train, sm_y_train)\n",
    "\n",
    "# Feature Importance of the Random Forest Model\n",
    "sm_log_reg_importance = sm_log_reg_model.coef_[0]\n",
    "sm_log_reg_importance = pd.DataFrame(sm_log_reg_importance, index=X.columns)\n",
    "sm_log_reg_importance.reset_index(inplace=True)\n",
    "sm_log_reg_importance.rename(columns={\n",
    "    \"index\": \"Feature\",\n",
    "    0: \"Score\"\n",
    "},\n",
    "    inplace=True)\n",
    "sm_log_reg_importance.sort_values(by=\"Score\", ascending=False, inplace=True)\n",
    "sm_log_reg_importance\n",
    "\n",
    "fig = px.bar(sm_log_reg_importance, x=\"Feature\", y=\"Score\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "sm_log_reg_pred = sm_log_reg_model.predict(sm_X_test)\n",
    "\n",
    "# Evaluating the predictions\n",
    "sm_log_reg_report = classification_report(sm_y_test, sm_log_reg_pred)\n",
    "print(sm_log_reg_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the F2 Score\n",
    "f2_score = fbeta_score(sm_y_test, sm_log_reg_pred, beta=0.5)\n",
    "f2_score = \"{:.5f}\".format(f2_score)\n",
    "print(\"F2 Score:\", f2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Confusion Matrix\n",
    "sm_log_reg_conf_mat = confusion_matrix(sm_y_test, sm_log_reg_pred)\n",
    "sm_log_reg_conf_mat = pd.DataFrame(sm_log_reg_conf_mat).reset_index(drop=True)\n",
    "sm_log_reg_conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the Confusion Matrix\n",
    "f, ax = plt.subplots()\n",
    "sns.heatmap(sm_log_reg_conf_mat,\n",
    "            annot=True,\n",
    "            linewidth=1.0,\n",
    "            fmt=\".0f\",\n",
    "            cmap=\"RdPu\",\n",
    "            ax=ax)\n",
    "plt.xlabel = (\"y_pred\")\n",
    "plt.ylabel = (\"y_true\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.2 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "dt_clf = DecisionTreeClassifier(random_state=24)\n",
    "sm_dt_model = dt_clf.fit(sm_X_train, sm_y_train)\n",
    "\n",
    "# Feature importances\n",
    "sm_dt_importance = sm_dt_model.feature_importances_\n",
    "sm_dt_importance = pd.DataFrame(sm_dt_importance,\n",
    "                                columns=[\"score\"]).reset_index()\n",
    "sm_dt_importance[\"Feature\"] = list(X.columns)\n",
    "sm_dt_importance.drop(columns=[\"index\"], inplace=True)\n",
    "sm_dt_importance.sort_values(by=\"score\",\n",
    "                             ascending=False,\n",
    "                             ignore_index=True,\n",
    "                             inplace=True)\n",
    "\n",
    "# Plotting the feature importances\n",
    "fig = px.bar(sm_dt_importance, x=\"Feature\", y=\"score\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "sm_dt_pred = sm_dt_model.predict(sm_X_test)\n",
    "\n",
    "# Evaluating the predictions\n",
    "sm_dt_report = classification_report(sm_y_test, sm_dt_pred)\n",
    "print(sm_dt_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the F2 Score\n",
    "f2_score = fbeta_score(sm_y_test, sm_dt_pred, beta=0.5)\n",
    "f2_score = \"{:.5f}\".format(f2_score)\n",
    "print(\"F2 Score:\", f2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Confusion Matrix\n",
    "sm_dt_conf_mat = confusion_matrix(sm_y_test, sm_dt_pred)\n",
    "sm_dt_conf_mat = pd.DataFrame(sm_dt_conf_mat).reset_index(drop=True)\n",
    "sm_dt_conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the Confusion Matrix\n",
    "f, ax = plt.subplots()\n",
    "sns.heatmap(sm_dt_conf_mat,\n",
    "            annot=True,\n",
    "            linewidth=1.0,\n",
    "            fmt=\".0f\",\n",
    "            cmap=\"RdPu\",\n",
    "            ax=ax)\n",
    "plt.xlabel = (\"y_pred\")\n",
    "plt.ylabel = (\"y_true\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.3 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forests\n",
    "rf_clf = RandomForestClassifier(random_state=24)\n",
    "sm_rf_model = rf_clf.fit(sm_X_train, sm_y_train)\n",
    "\n",
    "# Feature Importance of the Random Forest Model\n",
    "sm_rf_importance = sm_rf_model.feature_importances_\n",
    "sm_rf_importance = pd.DataFrame(sm_rf_importance,\n",
    "                                columns=[\"score\"]).reset_index()\n",
    "sm_rf_importance[\"Feature\"] = list(X.columns)\n",
    "sm_rf_importance.drop(columns=[\"index\"], inplace=True)\n",
    "sm_rf_importance.sort_values(by=\"score\",\n",
    "                             ascending=False,\n",
    "                             ignore_index=True,\n",
    "                             inplace=True)\n",
    "\n",
    "fig = px.bar(sm_rf_importance, x=\"Feature\", y=\"score\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "sm_rf_pred = sm_rf_model.predict(sm_X_test)\n",
    "\n",
    "sm_rf_report = classification_report(sm_y_test, sm_rf_pred)\n",
    "print(sm_rf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the F2 Score\n",
    "f2_score = fbeta_score(sm_y_test, sm_rf_pred, beta=0.5)\n",
    "f2_score = \"{:.5f}\".format(f2_score)\n",
    "print(\"F2 Score:\", f2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Confusion Matrix\n",
    "sm_rf_conf_mat = confusion_matrix(sm_y_test, sm_rf_pred)\n",
    "sm_rf_conf_mat = pd.DataFrame(sm_rf_conf_mat).reset_index(drop=True)\n",
    "sm_rf_conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the Confusion Matrix\n",
    "f, ax = plt.subplots()\n",
    "sns.heatmap(sm_rf_conf_mat,\n",
    "            annot=True,\n",
    "            linewidth=1.0,\n",
    "            fmt=\".0f\",\n",
    "            cmap=\"RdPu\",\n",
    "            ax=ax)\n",
    "plt.xlabel = (\"y_pred\")\n",
    "plt.ylabel = (\"y_true\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.4 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting model to the training data\n",
    "xgb_clf = XGBClassifier(random_state=24)\n",
    "sm_xgb_model = xgb_clf.fit(sm_X_train, sm_y_train)\n",
    "\n",
    "# Feature Importance of the Random Forest Model\n",
    "sm_xgb_importance = sm_xgb_model.feature_importances_\n",
    "sm_xgb_importance = pd.DataFrame(sm_xgb_importance,\n",
    "                                 columns=[\"score\"]).reset_index()\n",
    "sm_xgb_importance[\"Feature\"] = list(X.columns)\n",
    "sm_xgb_importance.drop(columns=[\"index\"], inplace=True)\n",
    "sm_xgb_importance.sort_values(by=\"score\",\n",
    "                              ascending=False,\n",
    "                              ignore_index=True,\n",
    "                              inplace=True)\n",
    "\n",
    "fig = px.bar(sm_xgb_importance, x=\"Feature\", y=\"score\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the predictions\n",
    "sm_xgb_pred = sm_xgb_model.predict(sm_X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "sm_xgb_report = classification_report(sm_y_test, sm_xgb_pred)\n",
    "print(sm_xgb_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the F2 Score\n",
    "f2_score = fbeta_score(sm_y_test, sm_xgb_pred, beta=0.5)\n",
    "f2_score = \"{:.5f}\".format(f2_score)\n",
    "print(\"F2 Score:\", f2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Confusion Matrix\n",
    "sm_xgb_conf_mat = confusion_matrix(sm_y_test, sm_xgb_pred)\n",
    "sm_xgb_conf_mat = pd.DataFrame(sm_xgb_conf_mat).reset_index(drop=True)\n",
    "sm_xgb_conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the Confusion Matrix\n",
    "f, ax = plt.subplots()\n",
    "sns.heatmap(sm_xgb_conf_mat,\n",
    "            annot=True,\n",
    "            linewidth=1.0,\n",
    "            fmt=\".0f\",\n",
    "            cmap=\"RdPu\",\n",
    "            ax=ax)\n",
    "plt.xlabel = (\"y_pred\")\n",
    "plt.ylabel = (\"y_true\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.5 CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initializing CatBoostClassifier\n",
    "catb_clf = CatBoostClassifier(metric_period=100, random_state=24)\n",
    "\n",
    "# Fitting it to the training data\n",
    "sm_catb_model = catb_clf.fit(sm_X_train, sm_y_train)\n",
    "\n",
    "# Feature Importance of the Model\n",
    "sm_catb_importance = sm_catb_model.feature_importances_\n",
    "sm_catb_importance = pd.DataFrame(sm_catb_importance,\n",
    "                                  columns=[\"score\"]).reset_index()\n",
    "\n",
    "sm_catb_importance[\"Feature\"] = list(X.columns)\n",
    "\n",
    "sm_catb_importance.drop(columns=[\"index\"], inplace=True)\n",
    "\n",
    "sm_catb_importance.sort_values(by=\"score\",\n",
    "                               ascending=False,\n",
    "                               ignore_index=True,\n",
    "                               inplace=True)\n",
    "\n",
    "fig = px.bar(sm_catb_importance, x=\"Feature\", y=\"score\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the predictions\n",
    "sm_catb_pred = sm_catb_model.predict(sm_X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "sm_catb_report = classification_report(sm_y_test,\n",
    "                                       sm_catb_pred,\n",
    "                                       target_names=[\"Stayed\", \"Churned\"])\n",
    "print(sm_catb_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the F2 Score\n",
    "f2_score = fbeta_score(sm_y_test, sm_catb_pred, beta=0.5)\n",
    "f2_score = \"{:.5f}\".format(f2_score)\n",
    "print(\"F2 Score:\", f2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Confusion Matrix\n",
    "sm_catb_conf_mat = confusion_matrix(sm_y_test, sm_catb_pred)\n",
    "sm_catb_conf_mat = pd.DataFrame(sm_catb_conf_mat).reset_index(drop=True)\n",
    "sm_catb_conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the Confusion Matrix\n",
    "f, ax = plt.subplots()\n",
    "sns.heatmap(sm_catb_conf_mat,\n",
    "            annot=True,\n",
    "            linewidth=1.0,\n",
    "            fmt=\".0f\",\n",
    "            cmap=\"RdPu\",\n",
    "            ax=ax)\n",
    "plt.xlabel = (\"y_pred\")\n",
    "plt.ylabel = (\"y_true\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.6 LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing LightGBM Classifier\n",
    "sm_lgb_clf = lgb.LGBMClassifier(random_state=24)\n",
    "\n",
    "# Fitting it to the training data\n",
    "sm_lgb_model = sm_lgb_clf.fit(sm_X_train, sm_y_train)\n",
    "\n",
    "# Feature Importance of the Model\n",
    "sm_lgb_importance = sm_lgb_model.feature_importances_\n",
    "sm_lgb_importance = pd.DataFrame(\n",
    "    sm_lgb_importance, columns=[\"score\"]).reset_index()\n",
    "sm_lgb_importance[\"Feature\"] = list(X.columns)\n",
    "sm_lgb_importance.drop(columns=[\"index\"], inplace=True)\n",
    "\n",
    "sm_lgb_importance.sort_values(by=\"score\",\n",
    "                              ascending=False,\n",
    "                              ignore_index=True,\n",
    "                              inplace=True)\n",
    "\n",
    "fig = px.bar(sm_lgb_importance, x=\"Feature\", y=\"score\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the predictions\n",
    "sm_lgb_pred = sm_lgb_model.predict(sm_X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "sm_lgb_report = classification_report(\n",
    "    sm_y_test, sm_lgb_pred, target_names=[\"Stayed\", \"Churned\"])\n",
    "print(sm_lgb_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the F2 Score\n",
    "f2_score = fbeta_score(sm_y_test, sm_lgb_pred, beta=0.5)\n",
    "f2_score = \"{:.5f}\".format(f2_score)\n",
    "print(\"F2 Score:\", f2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Confusion Matrix\n",
    "sm_lgb_conf_mat = confusion_matrix(sm_y_test, sm_lgb_pred)\n",
    "sm_lgb_conf_mat = pd.DataFrame(sm_lgb_conf_mat).reset_index(drop=True)\n",
    "sm_lgb_conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the Confusion Matrix\n",
    "f, ax = plt.subplots()\n",
    "sns.heatmap(sm_lgb_conf_mat,\n",
    "            annot=True,\n",
    "            linewidth=1.0,\n",
    "            fmt=\".0f\",\n",
    "            cmap=\"RdPu\",\n",
    "            ax=ax)\n",
    "plt.xlabel = (\"y_pred\")\n",
    "plt.ylabel = (\"y_true\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.7 Summarizing the Performance of the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_models = {\n",
    "    \"sm_lr\": sm_log_reg_model,\n",
    "    \"sm_dt\": sm_dt_model,\n",
    "    \"sm_rf\": sm_rf_model,\n",
    "    \"sm_xgb\": sm_xgb_model,\n",
    "    \"sm_catb\": sm_catb_model,\n",
    "    \"sm_lgb\": sm_lgb_model\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_models_eval = evaluation(smote_models, sm_X_test, sm_y_test)\n",
    "smote_models_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Standalone tree-based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the target & predictor variables\n",
    "st_X = train_data.drop(columns=[\"Churn\"])\n",
    "st_y = train_data[\"Churn\"]\n",
    "\n",
    "# Splitting the dataframe into train and test\n",
    "st_X_train, st_X_test, st_y_train, st_y_test = train_test_split(st_X, st_y, test_size= 0.25, random_state= 24, stratify= st_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the numeric columns\n",
    "st_scaler = MinMaxScaler()\n",
    "st_X_train[columns_to_scale] = st_scaler.fit_transform(st_X_train[columns_to_scale])\n",
    "st_X_test[columns_to_scale] = st_scaler.transform(st_X_test[columns_to_scale])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4.1 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "dt_clf = DecisionTreeClassifier(random_state= 24)\n",
    "dt_model = dt_clf.fit(st_X_train, st_y_train)\n",
    "\n",
    "# Feature importances\n",
    "dt_importance = dt_model.feature_importances_\n",
    "dt_importance = pd.DataFrame(dt_importance, columns= [\"score\"]).reset_index()\n",
    "dt_importance[\"Feature\"] = list(st_X.columns)\n",
    "dt_importance.drop(columns= [\"index\"], inplace=True)\n",
    "\n",
    "dt_importance.sort_values(by= \"score\",\n",
    "                          ascending= False,\n",
    "                          ignore_index= True,\n",
    "                          inplace= True)\n",
    "\n",
    "# Plotting the feature importances\n",
    "fig = px.bar(dt_importance, x= \"Feature\", y= \"score\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "dt_pred = dt_model.predict(st_X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "dt_report = classification_report(st_y_test, dt_pred, target_names= [\"Stayed\", \"Churned\"])\n",
    "print(dt_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the F2 Score\n",
    "f2_score = fbeta_score(st_y_test, dt_pred, beta= 0.5)\n",
    "f2_score = \"{:.5f}\".format(f2_score)\n",
    "print(\"F2 Score:\", f2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Confusion Matrix\n",
    "dt_conf_mat = confusion_matrix(st_y_test, dt_pred)\n",
    "dt_conf_mat = pd.DataFrame(dt_conf_mat).reset_index(drop= True)\n",
    "dt_conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the Confusion Matrix\n",
    "f, ax = plt.subplots()\n",
    "sns.heatmap(dt_conf_mat,\n",
    "            annot= True,\n",
    "            linewidth= 1.0,\n",
    "            fmt= \".0f\",\n",
    "            cmap= \"RdPu\",\n",
    "            ax= ax)\n",
    "plt.xlabel = (\"y_pred\")\n",
    "plt.ylabel = (\"y_true\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forests\n",
    "rf_clf = RandomForestClassifier(random_state= 24)\n",
    "rf_model = rf_clf.fit(st_X_train, st_y_train)\n",
    "\n",
    "# Feature Importance of the Random Forest Model\n",
    "rf_importance = rf_model.feature_importances_\n",
    "rf_importance = pd.DataFrame(rf_importance, columns= [\"score\"]).reset_index()\n",
    "rf_importance[\"Feature\"] = list(st_X.columns)\n",
    "rf_importance.drop(columns= [\"index\"], inplace= True)\n",
    "\n",
    "rf_importance.sort_values(by= \"score\",\n",
    "                          ascending= False,\n",
    "                          ignore_index= True,\n",
    "                          inplace= True)\n",
    "\n",
    "# Visualizing the feature importances\n",
    "fig = px.bar(rf_importance, x= \"Feature\", y= \"score\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "rf_pred = rf_model.predict(st_X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "rf_report = classification_report(st_y_test, rf_pred, target_names= [\"Stayed\", \"Churned\"])\n",
    "print(rf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the F2 Score\n",
    "f2_score = fbeta_score(st_y_test, rf_pred, beta= 0.5)\n",
    "f2_score = \"{:.5f}\".format(f2_score)\n",
    "print(\"F2 Score:\", f2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "rf_conf_mat = confusion_matrix(st_y_test, rf_pred)\n",
    "rf_conf_mat = (pd.DataFrame(rf_conf_mat).reset_index(drop=True)).rename(columns={0: \"Stayed\", 1: \"Churned\"})\n",
    "rf_conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualizing the Confusion Matrix\n",
    "f, ax = plt.subplots()\n",
    "sns.heatmap(rf_conf_mat,\n",
    "            annot= True,\n",
    "            linewidth= 1.0,\n",
    "            fmt= \".0f\",\n",
    "            cmap= \"RdPu\",\n",
    "            ax= ax)\n",
    "plt.xlabel = (\"y_pred\")\n",
    "plt.ylabel = (\"y_true\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4.3 Summarizing the results from the standalone tree-based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the dictionary for the results of the standalone tree-based models\n",
    "standalone_tree_models = {\"dt\": dt_model,\n",
    "                          \"rf\": rf_model\n",
    "                         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting the results of the standalone trees together\n",
    "standalone_tree_models_eval = evaluation(standalone_tree_models, st_X_test, st_y_test)\n",
    "standalone_tree_models_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Putting all the model summaries together for ease of selection\n",
    "all_models = pd.concat([oversampled_models_eval, undersampled_models_eval,\n",
    "                        smote_models_eval, standalone_tree_models_eval])\n",
    "\n",
    "# Sorting models by F2 score, F1 score and accuracy\n",
    "all_models = all_models.sort_values(by= [\"f2_score\", \"f1_weighted\", \"accuracy\"], ascending= False)\n",
    "all_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes on Features**\n",
    "\n",
    "From all the models, we note some consistency in the feature importances; we note that the top 5 most important features are:\n",
    "- Total Charges: increasing total charges lead to increased churn\n",
    "- Two-year contracts: 2-year contracts have a negative effect on churn; that is, those with 2-year contracts churned less\n",
    "- Monthly charges: increases in monthly charges lead to increased churn\n",
    "- Tenure: increasing tenure lead to lower churn.\n",
    "- One-year contracts: 1-year contracts have a negative effect on churn; that is, customers with 1-year contracts churned less"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.0 Model Optimization\n",
    "### Cross-Validation and Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Section 6.5 above, we note that the models from oversampling and SMOTE have the highest F2 and F1 scores, with the Oversampling Random Forest model ranking highest across all 17 models followed by 3 of 6 SMOTE models in the top 5. \n",
    "\n",
    "The standalone tree-based models are the worst performers, leaving the models based on the undersampling method in between.\n",
    "\n",
    "Based on the foregoing, the Random Forest will be chosen as the optimal model as it has the highest F2 score. Its F1 score is also high, implying that regardless of the weight of the precision and recall, it performs well. It will therefore be cross-validated and the hyperparameters tuned to optimize it and improve its performance.\n",
    "\n",
    "To ensure that no room is left to chance, the SMOTE XGBoost (2nd best model) will also be optimized to serve as a backup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Oversampling Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the best model, I will try K-fold cross-validation with different folds and different estimators (trees) to find the best number of estimators to use in getting the best version of the model.\n",
    "\n",
    "A range of 3 different folds and 10 estimators will be used for this assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pasting the model here for ease of access**\n",
    "\n",
    "*Oversampled Random Forest*\n",
    "```python\n",
    "rf_clf = RandomForestClassifier(random_state= 24)\n",
    "os_rf_model = rf_clf.fit(os_X_train, os_y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.1 K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Defining the number of folds for cross-validation and the range of estimators\n",
    "# cv = list(range(10, 21, 5))\n",
    "\n",
    "# # Using a loop to cross-validate with each number in the range of estimators\n",
    "# for c in cv:\n",
    "#     score = cross_val_score(estimator= os_rf_model,\n",
    "#                             X= os_X_train,\n",
    "#                             y= os_y_train,\n",
    "#                             cv= c\n",
    "#                            ).mean()\n",
    "#     print(f\"The average score after cross-validation for the model at {c} folds is:\", \"{0:.5}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, we note that generally the model performs well with an increasing number of estimators even though there is no clear pattern as results vary by the number of k-folds used for cross-validation.\n",
    "\n",
    "As such the number of estimators will be left open tuned along with other hyperparameters to find the best version of the model. For this, the RandomizedSearchCV and GridSearchCV will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.2 RandomizedSearch Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the random grid is defined by specifying some options for some hyperparameters for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Defining the values and instantiating the grid to be used in the RandomizedSearch\n",
    "# n_estimators = list(range(10, 1001, 50))\n",
    "# random_grid = {\"n_estimators\": n_estimators,\n",
    "#                \"max_depth\": [1, 5, 10, 20, 50, 75, 100, 150, 200, 300],\n",
    "#                \"bootstrap\": [True, False],\n",
    "#                \"criterion\": [\"gini\", \"entropy\"],\n",
    "#                \"max_features\": [\"sqrt\", \"log2\", None],\n",
    "#                \"random_state\": [24]\n",
    "#               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# # Running the RandomizedSearch Cross-Validation with the grid\n",
    "# rf_rscv = RandomizedSearchCV(estimator= os_rf_model,\n",
    "#                              param_distributions= random_grid,\n",
    "#                              n_iter= 30,\n",
    "#                              cv= 10,\n",
    "#                              random_state= 24,\n",
    "#                              n_jobs= -1)\n",
    "\n",
    "# # Fitting the model to the training data\n",
    "# rf_rscv.fit(os_X_train, os_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Looking at the best combination of hyperparameters for the model\n",
    "# best_params = rf_rscv.best_params_\n",
    "\n",
    "# print(\"The best combination of hyperparameters for the model will be:\")\n",
    "\n",
    "# for param_name in sorted(best_params.keys()):\n",
    "#     print(f\"{param_name}: {best_params[param_name]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Looking at the best score for the model during cross-validation\n",
    "# print(\"The mean cross-validated score of the model's best combination of hyperparameters is:\",\n",
    "#       \"{0:.5}\".format(rf_rscv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the RandomizedSearchCV, we note a significant improvement in the score of the model. As such, we will build an \"optimized\" version of the model using the recommended parameters from above and assess it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining an optimized version of the model with the best parameters\n",
    "best_rf_model_rscv = RandomForestClassifier(bootstrap= False,\n",
    "                                            criterion= \"gini\",\n",
    "                                            max_depth= 20,\n",
    "                                            max_features= \"log2\",\n",
    "                                            n_estimators= 510,\n",
    "                                            random_state= 24\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model to the training data\n",
    "best_rf_model_rscv = best_rf_model_rscv.fit(os_X_train, os_y_train)\n",
    "\n",
    "# Predicting the test data\n",
    "best_rf_pred = best_rf_model_rscv.predict(os_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "best_rf_report = classification_report(os_y_test, best_rf_pred, target_names= [\"Stayed\", \"Churned\"])\n",
    "print(best_rf_report)\n",
    "\n",
    "# Calculating the accuracy score\n",
    "accuracy = accuracy_score(os_y_test, best_rf_pred)\n",
    "accuracy = \"{:.5f}\".format(accuracy)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Calculating the F2 Score\n",
    "f2_score = fbeta_score(os_y_test, best_rf_pred, beta= 0.5)\n",
    "f2_score = \"{:.5f}\".format(f2_score)\n",
    "print(\"F2 Score:\", f2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "best_rf_conf_mat = confusion_matrix(os_y_test, best_rf_pred)\n",
    "best_rf_conf_mat = (pd.DataFrame(best_rf_conf_mat).reset_index(drop= True)).rename(columns= {0: \"Stayed\", 1: \"Churned\"})\n",
    "best_rf_conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the Confusion Matrix\n",
    "f, ax = plt.subplots()\n",
    "sns.heatmap(best_rf_conf_mat,\n",
    "            annot= True,\n",
    "            linewidth= 1.0,\n",
    "            fmt= \".0f\",\n",
    "            cmap= \"RdPu\",\n",
    "            ax= ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the scores and confusion matrix above, we see that this version of the model is the best version we can have for now. With an average (cross-validated) score of about 89% and great F1-(about 90%) and F2-(about 88%) scores, we are almost certain that this model will accurately predict which customers are likely to churn and enable Vodafone take active steps to improve customer retention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.3 GridSearch Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Defining the parameter grid for the GridsearchCV (chosen with reference to the best estimators from the RandomizedSearchCV)\n",
    "# gscv_param_grid = {\"n_estimators\": [100, 200, 300, 400, 500, 600],\n",
    "#                    \"max_features\": [\"sqrt\", \"log2\"],\n",
    "#                    \"max_depth\": [10, 20, 40, 80, 160],\n",
    "#                    \"criterion\": [\"gini\"], \n",
    "#                    \"random_state\": [24], \n",
    "#                    \"bootstrap\": [False]\n",
    "#                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Executing GridSearchCV\n",
    "# rf_gscv = GridSearchCV(estimator= os_rf_model,\n",
    "#                        param_grid= gscv_param_grid,\n",
    "#                        n_jobs= -1,\n",
    "#                        cv= 10)\n",
    "\n",
    "# # Fitting the model to the training data\n",
    "# rf_gscv.fit(os_X_train, os_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Printing the best combination of hyperparameters for the model\n",
    "# best_params = rf_gscv.best_params_\n",
    "# print(\"The best combination of hyperparameters for the model will be:\")\n",
    "\n",
    "# for param_name in sorted(best_params.keys()):\n",
    "#     print(f\"{param_name}: {best_params[param_name]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Looking at the best score for the model during cross-validation\n",
    "# print(\"The mean cross-validated score of the model's best combination of hyperparameters is:\",\n",
    "#       \"{0:.5}\".format(rf_gscv.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Building a new model with the best parameters\n",
    "# gscv_best_rf = RandomForestClassifier(random_state= 24,\n",
    "#                                       bootstrap= False,\n",
    "#                                       max_features= \"sqrt\",\n",
    "#                                       n_estimators= 500,\n",
    "#                                       max_depth= 20,\n",
    "#                                       criterion= \"gini\"\n",
    "#                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Fitting the optimized model to the training data\n",
    "# gscv_best_rf.fit(os_X_train, os_y_train)\n",
    "\n",
    "# # Predicting the test data\n",
    "# gscv_rf_pred = gscv_best_rf.predict(os_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluating the model\n",
    "# gscv_rf_report = classification_report(os_y_test, gscv_rf_pred, target_names= [\"Stayed\", \"Churned\"])\n",
    "# print(gscv_rf_report)\n",
    "\n",
    "# # Calculating the accuracy score\n",
    "# accuracy = accuracy_score(os_y_test, gscv_rf_pred)\n",
    "# accuracy = \"{:.5f}\".format(accuracy)\n",
    "# print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# # Calculating the F2 Score\n",
    "# f2_score = fbeta_score(os_y_test, gscv_rf_pred, beta= 0.5)\n",
    "# f2_score = \"{:.5f}\".format(f2_score)\n",
    "# print(\"F2 Score:\", f2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Confusion Matrix\n",
    "# gscv_rf_conf_mat = confusion_matrix(os_y_test, gscv_rf_pred)\n",
    "# gscv_rf_conf_mat = (pd.DataFrame(gscv_rf_conf_mat).reset_index(drop= True)).rename(columns= {0: \"Stayed\", 1: \"Churned\"})\n",
    "# gscv_rf_conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualizing the Confusion Matrix\n",
    "# f, ax = plt.subplots()\n",
    "# sns.heatmap(gscv_rf_conf_mat,\n",
    "#             annot= True,\n",
    "#             linewidth= 1.0,\n",
    "#             fmt= \".0f\",\n",
    "#             cmap= \"RdPu\",\n",
    "#             ax= ax\n",
    "#            )\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 SMOTE XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As indicated earlier, the second best performing model will also be optimized to ensure that at the end of the day, there are at least two optimized models from which to choose. The XGBoost models will therefore be optimized with K-FOld Cross-Validation and/or RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Pasting the model here for ease of access*\n",
    "\n",
    "**SMOTE XGBoost Classifier**\n",
    "```python\n",
    "xgb_clf = XGBClassifier(random_state= 24)\n",
    "sm_xgb_model = xgb_clf.fit(sm_X_train, sm_y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.1 K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As was done with the Random Forest model, the XGBoost Classifier is is cross-validated using K-Fold Cross-Validation with 3 different k-values and 10 different estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Defining the number of folds for cross-validation and the range of estimators\n",
    "# cv = list(range(10, 21, 5))\n",
    "# n_estimators = list(range(10, 101, 10))\n",
    "\n",
    "# # Defining a loop to cross-validate with each number in the range of estimators\n",
    "# for c in cv:\n",
    "#     print(f\"The model's average score after cross-validation at {c} folds is:\")\n",
    "#     score = cross_val_score(estimator= sm_xgb_model,\n",
    "#                             X= sm_X_train, y= sm_y_train,\n",
    "#                             cv= c\n",
    "#                             ).mean()\n",
    "#     print(\"score_\" + str(c) + \"_folds:\", \"{0:.5}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results above, we note that the best performance on each iteration is best at 100 estimators, with mean score on cross-validation generally increasing with the number of estimators. \n",
    "\n",
    "Since it is the only hyperparameter we tuned here, we may use these findings to inform further tuning and model optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.2 RandomizedSearch Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Defining the values for the RandomizedSearchCV\n",
    "# random_grid = {\"colsample_bytree\": [0.1, 0.3, 0.5, 0.7],\n",
    "#                \"learning_rate\": [0.1, 0.3, 0.5, 0.7, 1.0],\n",
    "#                \"max_depth\": [5, 10, 15, 20, 25, 30, 35],\n",
    "#                \"booster\": [\"gbtree\", \"gblinear\", \"dart\"],\n",
    "#                \"n_estimators\": [5, 10, 20, 50, 80, 100]\n",
    "#                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# # Running the RandomizedSearch Cross-Validation with the above set of Parameters\n",
    "# xgb_rs_cv_model = RandomizedSearchCV(estimator= sm_xgb_model,\n",
    "#                                      param_distributions= random_grid,\n",
    "#                                      n_iter= 30,\n",
    "#                                      cv= 10,\n",
    "#                                      random_state= 24,\n",
    "#                                      n_jobs= -1)\n",
    "\n",
    "# # Fitting the model to the training data\n",
    "# xgb_rs_cv_model.fit(sm_X_train, sm_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Looking at the best combination of hyperparameters for the model\n",
    "# best_params = xgb_rs_cv_model.best_params_\n",
    "# print(\"The best combination of hyperparameters for the model will be:\")\n",
    "# for param_name in sorted(best_params.keys()):\n",
    "#     print(f\"{param_name} : {best_params[param_name]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Looking at the best score for the model during cross-validation\n",
    "# print(\"The model's cross-validated score with the best combination of hyperparameters is:\", \n",
    "#       \"{0:.5}\".format(xgb_rs_cv_model.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the best version of the model with the best parameters\n",
    "best_xgb_model = XGBClassifier(random_state= 24,\n",
    "                               booster= \"dart\",\n",
    "                               colsample_bytree= 0.5,\n",
    "                               learning_rate= 0.5,\n",
    "                               max_depth= 20,\n",
    "                               n_estimators= 80\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model to the training data\n",
    "best_xgb_model = best_xgb_model.fit(sm_X_train, sm_y_train)\n",
    "\n",
    "# Predicting the test data\n",
    "best_xgb_pred = best_xgb_model.predict(sm_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "best_xgb_report = classification_report(sm_y_test, best_xgb_pred, target_names= [\"Stayed\", \"Churned\"])\n",
    "print(best_xgb_report)\n",
    "\n",
    "# Calculating the accuracy score\n",
    "accuracy = accuracy_score(sm_y_test, best_xgb_pred)\n",
    "accuracy = \"{:.5f}\".format(accuracy)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Calculating the F2 Score\n",
    "f2_score = fbeta_score(sm_y_test, best_xgb_pred, beta= 0.5)\n",
    "f2_score = \"{:.5f}\".format(f2_score)\n",
    "print(\"F2 Score:\", f2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "best_xgb_conf_mat = confusion_matrix(sm_y_test, best_xgb_pred)\n",
    "best_xgb_conf_mat = (pd.DataFrame(best_xgb_conf_mat).reset_index(drop= True)).rename(columns={0: \"Stayed\", 1: \"Churned\"})\n",
    "best_xgb_conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the Confusion Matrix\n",
    "f, ax = plt.subplots()\n",
    "sns.heatmap(best_xgb_conf_mat,\n",
    "            annot= True,\n",
    "            linewidth= 1.0,\n",
    "            fmt= \".0f\",\n",
    "            cmap= \"RdPu\",\n",
    "            ax= ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the confusion matrix after optimization is compared to the original confusion matrix from the original model in Section 6.3.4, we note that there is no difference between the performance of the model, hence we go with the best model(from 7.1 above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.0 Future Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As concluded in Section 7 above, the selected model to be used for the predictions will be the Random Forest from the Oversampling method. As such, the data on which predictions are to be made will be oversampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the model (for shege reasons)\n",
    "best_rf_model_rscv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversampling the test data and confirming the shape\n",
    "final_test_data = test_data.copy()\n",
    "count_not_churned, count_churned = final_test_data[\"Churn\"].value_counts()\n",
    "count_not_churned, count_churned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the observations for the 2 classes\n",
    "test_not_churned = final_test_data[final_test_data[\"Churn\"] == 0]\n",
    "test_churned = final_test_data[final_test_data[\"Churn\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversampling the churned class and combining with the original dataframe\n",
    "test_churn_oversampled = test_churned.sample(count_not_churned, replace= True)\n",
    "test_oversampled = pd.concat([test_not_churned, test_churn_oversampled])\n",
    "\n",
    "print(\"Random over-sampling:\")\n",
    "print(test_oversampled[\"Churn\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the target & predictor variables\n",
    "test_X = test_oversampled.drop(columns= [\"Churn\"])\n",
    "test_y = test_oversampled[\"Churn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the test data columns\n",
    "test_X[columns_to_scale] = os_scaler.transform(test_X[columns_to_scale])\n",
    "test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the test data\n",
    "best_rf_pred = best_rf_model_rscv.predict(test_X)\n",
    "\n",
    "# Evaluating the model\n",
    "best_rf_report = classification_report(test_y, best_rf_pred, target_names= [\"Stayed\", \"Churned\"])\n",
    "print(best_rf_report)\n",
    "\n",
    "# Calculating the F2 Score\n",
    "f2_score = fbeta_score(test_y, best_rf_pred, beta= 0.5)\n",
    "f2_score = \"{:.5f}\".format(f2_score)\n",
    "print(\"F2 Score:\", f2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "best_rf_conf_mat = confusion_matrix(test_y, best_rf_pred)\n",
    "best_rf_conf_mat = (pd.DataFrame(best_rf_conf_mat).reset_index(drop= True)).rename(columns= {0: \"Stayed\", 1: \"Churned\"})\n",
    "best_rf_conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the Confusion Matrix\n",
    "f, ax = plt.subplots()\n",
    "sns.heatmap(best_rf_conf_mat,\n",
    "            annot= True,\n",
    "            linewidth= 1.0,\n",
    "            fmt= \".0f\",\n",
    "            cmap= \"RdPu\",\n",
    "            ax= ax\n",
    "           )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A look at the parameters of the final XGB model\n",
    "best_xgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a copy of the the training dataframe for the SMOTE\n",
    "smote_test = test_data.copy()\n",
    "\n",
    "sm_X = smote_test.drop(columns= [\"Churn\"])\n",
    "sm_y = smote_test[\"Churn\"]\n",
    "\n",
    "# Resampling the dataframe using SMOTE\n",
    "smote = SMOTE(sampling_strategy= \"minority\")\n",
    "smote_X, smote_y = smote.fit_resample(sm_X, sm_y)\n",
    "smote_y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the numeric columns\n",
    "smote_X[columns_to_scale] = sm_scaler.transform(smote_X[columns_to_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the test data\n",
    "final_xgb_pred = best_xgb_model.predict(smote_X)\n",
    "\n",
    "# Evaluating the model\n",
    "final_xgb_report = classification_report(smote_y, final_xgb_pred, target_names= [\"Stayed\", \"Churned\"])\n",
    "print(final_xgb_report)\n",
    "\n",
    "# Calculating the F2 Score\n",
    "f2_score = fbeta_score(smote_y, final_xgb_pred, beta= 0.5)\n",
    "f2_score = \"{:.5f}\".format(f2_score)\n",
    "print(\"F2 Score:\", f2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "final_xgb_conf_mat = confusion_matrix(smote_y, final_xgb_pred)\n",
    "final_xgb_conf_mat = (pd.DataFrame(final_xgb_conf_mat).reset_index(drop= True)).rename(columns= {0: \"Stayed\", 1: \"Churned\"})\n",
    "final_xgb_conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the Confusion Matrix\n",
    "f, ax = plt.subplots()\n",
    "sns.heatmap(final_xgb_conf_mat,\n",
    "            annot= True,\n",
    "            linewidth= 1.0,\n",
    "            fmt= \".0f\",\n",
    "            cmap= \"RdPu\",\n",
    "            ax= ax\n",
    "           )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.0 Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Summary of Key Insights and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Most of the churned customers had tenures within 1 - 29 months, and customers who passed the 29-month mark generally churned less.\n",
    "2. Customers generally churned more when they crossed USD 56.15 monthly charge mark, with majority falling between USD 56.15 (lower quartile) and USD 79.65 (median monthly charge of churned customers)\n",
    "3. Churn levels spiked most when monthly charges paid USD 64.45\n",
    "4. Most of the customers who churned fell within USD 134.46 and USD 2,332.30. This is surprising and may be investigated further as it gives an indication that total charges may not be the sole reason for churn\n",
    "5. Males are almost just as likely to churn as females. Hence, gender - like total charges - may not be a sole determinant for assessing the likelihood of churn\n",
    "6. Customers without partners were about 67% more likely to churn that those with partners.\n",
    "7. Fibre-optic service users were over twice as likely to churn as compared to DSL users (41.89% vs. 19.00%). Given that, ideally, fibre-optic is supposed to be faster (and better) than DSL, the churn proportion there was particularly worrying. Vodafone may want to investigate the reasons for high churn among customers. It may also review it's fibre-optic internet services and reach out to customers for more information.\n",
    "8. Over 63% of internet service users did not use online security services. Since customers in this group were about 42% likely to churn, Vodafone may consider increased promotion for their online security services, as user of online security services did not churn much.\n",
    "9. Less than 29% of customers used the Tech Support services and were 41.65% likely to churn. Given that tech support is critical to tech service delivery, Vodafone may want to bundle tech support offerings with other services to ensure that customers receive support as and when needed, and churn is reduced.\n",
    "10. Internet service users who streamed (either TV or movies) were just as likely to churn as those who did not. This is concerning and raises questions about the streaming services offered by Vodafone. The company may want to evaluate their streaming services and ensure engagement with customers to improve the streaming service delivery and reduce churn among streamers. Improving this will mean that any customer who signs unto streaming services will be unlikely to churn.\n",
    "11. The churn proportion for electronic checks (45.29%) is concerning, and should be investigated and improved to ensure convenience and ease of use for customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per their confusion matrices and F2 scores, the XGBoost model (0.79 F2-score) generalizes and performs better on unseen data than the Random Forest model (0.69 F2-score). The XGBoost is therefore recommended for further optimization and deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 Exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the requirements\n",
    "requirements = \"\\n\".join(f\"{m.__name__}=={m.__version__}\" for m in globals().values() if getattr(m, \"__version__\", None))\n",
    "\n",
    "with open(\"requirements.txt\", \"w\") as f:\n",
    "    f.write(requirements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary of objects to export\n",
    "exports = {\"encoder\": oh_encoder,\n",
    "           \"scaler\": sm_scaler,\n",
    "           \"model\": best_xgb_model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the dictionary with Pickle\n",
    "with open(\"Gradio_App_toolkit\", \"wb\") as file:\n",
    "    pickle.dump(exports, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the model\n",
    "best_xgb_model.save_model(\"xgb_model.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "78px",
    "width": "187px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "541.6px",
    "left": "278px",
    "top": "110.325px",
    "width": "239.819px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "1a4ce4bc5f820c6c47c7565419227e532b3448deb4a621e77e51010fbe64b648"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
